We thank the reviewers for their insightful comments, and valuable
suggestions. An explanation to address some of the critical comments
is that unfortunately, due to a LaTeX typo, Figure 4 in the paper is a
duplicate of Figure 2, and so the key contrast between the first and
second experiment was unfortunately lost. We apologize for this
oversight. In addition, there appears to be a misunderstanding about
the properties of the prior on the transition matrix. We address these
two issues below. Finally, the reviewers offer excellent suggestions
for ways to extend and further generalize our model, some of which we
are already working on. However, the existing model is useful for an 
important class of time series inference problems.

As mentioned, Figure 4 erroneously duplicates Figure 2, and hence it
appeared that our model uses more states than an ordinary HDP-HMM,
even on HDP-HMM data. Reviewer 1 worries whether the results
indicate whether local transitions are present, and Reviewer 2 notes
that "the LT model achieves a similar log likelihood to the non LT
model but uses many more states." Based on the incorrect figure, these
are natural concerns. What the correct figure shows is
that our model discovers the same number of states as the simpler
model (about 5, which is the actual number in the generated data), and
the posterior distribution for lambda places most of its mass near 0, 
in sharp contrast to the analogous figure in
the diarization experiment, where the posterior mean is
above 1.5.  Since our model reduces to an 
HDP-HMM when lambda is 0, we argue that it can be used in settings
where local transition behavior is suspected, but not certain to be 
present. Again, we apologize for our mistake. 
Though we do not yet have results for other
kinds of latent state spaces, we have run experiments on
several additional diarization and synthetic datasets, varying the
number of states, dimensions, and amount of noise,
and find a robust advantage of our model when local transitions
are present, and indistinguishable results (including number of states
used, and lambda values near zero) when they are not. We will include
more thorough results in the next version.  

Transition matrix prior: Reviewer 3 comments that 'While the current model
assumes that a transition from \theta_i to \theta_j is equally
probable to a transition from \theta_j to \theta_i ..., some state
might be a "ground" state that can be reached from many states
easily.' However, our model does not assume that the transition matrix
itself is symmetric; only that the similarity matrix used to rescale
the HDP-generated transition matrix is symmetric. (We will make this
clearer in the next version of our paper.) If there are
ground states easily reached from many other states, our model can
find this by assigning a high weight to such states in the
top level of the HDP. The possibility of such behavior is
reason to use a kernel that does not decay too rapidly for large
distances (such as the exponential we use, or maybe
one with hinges that is constant past a certain distance), 
so that the HDP can still override the LT component as
needed. 

Finally, the reviewers point out some excellent extensions and additional
comparisons. Reviewer 3 notes that the infinite Factorial HMM would be 
a good model to compare against, instead of the finite 
Factorial HMM. We can include this; however, since the (finite) 
FHMM we did compare against was given the correct dimension 
up front, it should have an advantage compared to an iFHMM.  
We note that where the HDP-HMM is missing the concept of 
"local transitions" possessed by 
the Factorial models and ours, the vanilla (i)FHMM, due to prior
independence among chains, is missing the capacity, possessed by
the HDP-HMM and our model, to discover a sparse subspace of the 
full combinatorial state space. As an aside, it would be conceptually 
straightforward to incorporate an IBP into the state prior 
in our model to accommodate an unbounded number of latent
dimensions. We will add a discussion of these properties to the next
version, and possibly a table summarizing the differences.

Reviewer 3 also makes the excellent suggestion of putting a GP prior
on the rescaling factors, allowing the model to learn
the topology of the latent state space. We agree that this
would be an exciting generalization, and it is on
our (long) list of future "features". Still, we stress that there
are applications (such as speaker diarization) where
it is clear what constitutes a "local" transition --- namely,
the latent features overlap. This is a key difference between the
non-sequential DILN model and our model: in the HMM setting, instead
of "distances" being between entities of two different kinds (e.g.,
topics and documents), they are between like entities (latent
states), and it is clearer what those distances should look like.
