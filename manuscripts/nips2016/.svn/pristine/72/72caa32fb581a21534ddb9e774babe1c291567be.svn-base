\section{Background and Related Work}

The conventional Hierarchical Dirichlet Process Hidden Markov Model
(HDP-HMM) \cite{beal2001infinite, teh2006hierarchical} 
is a prior distribution on the transition matrix of a Hidden Markov
Model with a countably infinite state space.  The rows of the
infinite matrix are coupled through their dependence on a common, discrete base measure,
itself drawn from a Dirichlet Process (DP).
The hierarchical structure ensures that, despite the infinite state
space, a common set of destination states will be reachable with high probability
from each source state.  The generative process for the HDP-HMM is the following:

Each of a countably infinite set of states, indexed by $j$, has parameters,
$\theta_j$, drawn from a base measure, $H$.  A top-level
set of state weights, $\bbeta = (\beta_1, \beta_2, \dots)$, is drawn from a stick-breaking
process ($\mathsf{GEM}$) with concentration parameter $\gamma > 0$.
\begin{equation}
\theta_j \stackrel{i.i.d.}{\sim} H \qquad \bbeta \sim \mathsf{GEM}(\gamma)
\end{equation}
The actual transition distribution, $\bpi_j$, from state $j$,
is drawn from a DP with concentration $\alpha$ and base measure $\bbeta$:
\begin{equation}
  \label{eq:1}
  \bpi_j \stackrel{i.i.d}{\sim} DP(\alpha \bbeta) \qquad j = 1, 2, \dots
\end{equation}
The hidden state sequence is then generated according to the $\pi_j$.
Let $z_t$ be the index of the chain's state at time $t$.  Then we have
\begin{equation}
  \label{eq:4}
  z_t \given z_{t-1}, \bpi_{z_{t-1}} \sim \bpi_{z_{t-1}} \qquad t = 1, 2, \dots, T
\end{equation}
where $T$ is the length of the data sequence.  Finally, the emission distribution 
for state $j$ is a function of $\theta_j$, so that we have
\begin{equation}
  \label{eq:5}
  y_t \given z_{t}, \theta_{z_t} \sim F(\theta_{z_t})
\end{equation}

A shortcoming of this model is that the generative process does not
take into account the fact that the set of source states is the same
as the set of destination states: that is, the distribution $\bpi_j$
has an element which corresponds to state $j$.  Put another way, there
is no special treatment of the diagonal of the transition matrix, so
that self-transitions are no more likely {\it a priori} than
transitions to any other state.  The Sticky HDP-HMM \cite{fox2008hdp}
addresses this issue by adding an extra mass at location $j$ to the base
measure of the DP that generates $\bpi_j$.  That is, \eqref{eq:1} is replaced by
\begin{equation}
  \label{eq:6}
  \bpi_j \sim DP(\alpha\bbeta + \kappa \delta_j).
\end{equation}
An alternative approach that treats self-transitions as special 
is the HDP Hidden Semi-Markov Model (HDP-HSMM)
\cite{johnson2013bayesian}, wherein state duration distributions are modeled
separately, and ordinary self-transitions are ruled out.  In both the
Sticky HDP-HMM and the HDP-HSMM, auxiliary latent variables are introduced to simplify
conditional posterior distributions and facilitate Gibbs sampling.
However, while both of these models have the ability to privilege
self-transitions, they contain no notion of
similarity for pairs of states that are not identical: 
in both cases, when the transition matrix
is integrated out, the prior probability of
transitioning to state $j'$ depends only on the top-level stick
weight associated with state $j'$, and not on the identity or
parameters of the previous state $j$.

The present paper makes two main contributions: first we 
define the HDP-HMM with Local Transitions (HDP-HMM-LT), in which a
similarity structure is defined on the state space, and ``similar''
states are {\em a priori} more likely to have transitions between
them.  This is accomplished by elementwise rescaling and then
renormalizing the HDP transition matrix.  A similar rescaling and
renormalization approach is taken in the Discrete Infinite Logistic
Normal (DILN) model \cite{paisley2012discrete} in the setting of topic
modeling.  There, however, the contexts and the mixture components
(topics) are distinct sets, and there is no notion of temporal
dependence.  The second contribution of the present paper 
is an augmented data technique that allows Gibbs sampling to be used.
Previous rescaled HDP models such as DILN have relied on variational
inference.

The paper is structured as follows: In section \ref{sec:model} we
define the HDP-HMM-LT.  In section \ref{sec:inference}, we develop a
straightforward Gibbs sampling algorithm 
based on an augmented data representation, the Markov
Jump Process with Failed Transitions.  In section
\ref{sec:experiments} we test the model and inference algorithm 
on synthetic data generated from similar generative models, as well as
on a speaker diarization task in which the speakers are
inter-dependent.  Finally, in section \ref{sec:discussion}, we 
conclude and discuss the relationships between the HDP-HMM-LT and
existing HMM variants.