\section{Discussion}
\label{sec:discussion}

We have defined a new probabilistic model, 
the Hierarchical Dirichlet Process Hidden Markov Model
with Local Transitions (HDP-HMM-LT), which generalizes the HDP-HMM by
encoding prior information about state space geometry
via a similarity kernel, making transitions between ``nearby''
pairs of states more likely {\em a priori}.  
By introducing an augmented data representation in the form of a
Markov Jump Process with Failed Transitions, we obtain
a simple Gibbs sampling algorithm for both the LT and
ordinary HDP-HMM.  
When multiple latent chains are interdependent, the HDP-HMM-LT model
combines the HDP-HMM's capacity to discover a suitable number of joint states
with the Factorial HMM's ability to encode the property that most transitions involve
a small number of chains.  The HDP-HMM-LT outperforms both on
a speaker diarization task in which speakers perform conversational
groups.  Despite the addition of the similarity
kernel, the HDP-HMM-LT is able to suppress its local
transition prior when the data does not support it, achieving
identical performance to the HDP-HMM on data generated directly from the latter.

Although the local transition property
is particularly clear when changes in state occur at
different times for different latent features, as with binary
vector-valued states, it is worth noting that
the model can be used with any state space equipped with a suitable similarity kernel
(in fact, similarities need not be defined in
terms of emission parameters as they are here; state ``locations''
could be represented and inferred separately as in the Discrete Infinite Logistic Normal model
\cite{paisley2012discrete}).  Furthermore, although for simplicity we have focused
on fixed-dimension binary vectors, it would be
relatively straightforward to add the LT property to a model in which
the latent states consist (say) of infinite binary vectors, such as
the Infinite Factorial Hidden Markov Model
\cite{gael2009infinite} and the Infinite Factorial Dynamic Model
\cite{valera2015infinite}, which employ the Indian Buffet Process 
\cite{ghahramani2005infinite}.  The similarity kernel used
here could be employed without any change there: since all but finitely many
coordinates are zero in the Indian Buffet Process,
the distance between any two states is finite.
