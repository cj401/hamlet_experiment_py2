We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between ``similar'' states.  This is accomplished by defining a similarity kernel on the state space, and scaling transition probabilities by pairwise similarities. This induces a global correlation structure over the transition probabilities based on the topology induced by the similarity kernel.  We call this model the Hierarchical Dirichlet Process Hidden Markov Model with Local Transitions (HDP-HMM-LT). Unfortunately the conditional posterior of the transition distributions are no longer conjugate, due to the varying scale parameters, and so we present an alternative representation of this process as the marginalization of a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. When holding times and failed transitions are reintroduced as auxiliary data, conditional conjugacy is restored, admitting exact Gibbs sampling.  Even without the LT modification, conditioning on the holding times simplifies inference for the concentration parameters of the HDP, and allows immediate generalization to Semi-Markov dynamics without additional data augmentation.  We evaluate the model and inference method on a collection of speaker diarization data sets in which speakers form conversational groups, so that there is prior dependence among chains, but most transitions involve only one or two chains at a time (transitions are local).  Our model compares favorably to both the the HDP-H(S)MM and Binary Factorial HMM without suffering in performance when the data is generated directly from the comparison models.
