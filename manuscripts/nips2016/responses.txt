We are grateful to the reviewers for their many helpful comments and suggestions.  We first reply to the concerns raised by multiple reviewers, and then turn to points raised by individual reviewers.  

* R2, R4 and R7 ask about computational complexity. The auxiliary variables in the "failed jumps" representation require O(J^2) computation (J=the number of states considered), but the overall complexity is the same as the vanilla HDP-HMM, due to the message-passing bottleneck, which is O(TJ^2). In Exp. 1 with T=400 and J=600, our C++ implementation took 1-2 s/iteration, with room for optimization. We have also implemented a beam-sampler to sample the z_{t}, which controls the complexity in J, at the cost of greater autocorrelation between iterations. We will add this information.

* R1 and R3 ask when the series in (8) converges. Briefly, finiteness is guaranteed by the fact that the sum of the un-rescaled transition rates has a Gamma(\alpha, 1) distribution, and since phi is bounded, the sum of the scaled rates is at most a constant multiple of this Gamma variate, hence finite a.s. We will add a more detailed proof.

* In general, we agree with the reviewers who suggest that it would be useful to include additional derivations in a supplement.

* R2, R3 and R4 ask about constraints on the similarity kernel. We assume phi <= 1, and although our derivation assumes symmetry, this only enters into the posterior for theta, which could easily be re-derived if symmetry is not desired. We also assume self-similarity is 1, guaranteeing normalizability, but one could achieve that another way. We will be more precise about these constraints. In our experiments, theta determines phi, but the relationship could be probabilistic, or non-existent. In a model variant not discussed in the paper, states have latent "locations" governing similarity that are not connected to theta at all. We sample locations jointly using HMC.

* R2,3 and 4 request source code. We are in the process of improving documentation and better encapsulating dependencies, and will make the code available. We regret that it was not in a useful form by submission time.

* Several reviewers wonder about real-world applicability. The motivation for this model was the flow of topics across sentences, where a sentence can be associated with arbitrarily many topics, and where we expect transitions between binary topic vectors to be "local". Sadly, feature-engineering lagged model-development, and we were unable to include results on this real data in the paper. We hope to publish this application, and another on learning musical grammar, separately, as we expect each to require quite a bit of background description, but we will better motivate the model, at least, in revision. We hope to have results soon on the REDD power disaggregation dataset, and would appreciate suggestions for other existing data that would not require too much background.

* R2 and 4 note that Eq. 10 is confusing. This is indeed meant to be a likelihood, not a posterior; we apologize for the inconsistent notation and will correct this. R4 is correct that the (1-phi_{jj'}) in the exponent cancels: specifically, with the exponential term in the Gamma distribution of u_j. We will flesh out this derivation.

* R1 notes connections to correlated and dependent random measures.  This work is certainly relevant.  We will certainly incorporate appropriate references.

* R4 asks what fixing the weight matrix entails. We fixed only the linear map from latent states to means in observation space, which is constant across states (only the binary vectors themselves differ). The number of latent states was not fixed.

* R4 asks why lambda in Exp. 2 does not go to zero. There is quite a bit of mass closer to zero than the 0.1 or 0.2 mean value, but since larger values of lambda are visited occasionally, the mean is not zero. Posterior quantiles may be more informative than frequentist confidence intervals here.

* R6 makes the good suggestion that a variational inference scheme could likely also be developed.  We agree this is an important avenue for future work, but due to time and space constraints, did not develop it here.

* R7 asks what drives the large gap in number of states used between LT and noLT models in Exp. 1.  We like the suggestion for an initialization experiment, but we believe the gap is due to the noLT model collapsing many states with similar emission distributions into one, while the LT model, in which transitions between such similar states are a priori probable, distinguishes them.  With 2^16 possible binary vectors, each state has a large number of such "neighbors", so we are not too surprised by a 3- to 4-fold difference.

* We like R7's suggestions for additional visualizations.

Again, we are grateful to the reviewers for their effort, catching our typos and notational inconsistencies, and for their suggestions to improve the clarity of the presentation.
