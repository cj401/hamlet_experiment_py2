We describe a generalization of the Hierarchical Dirichlet Process
Hidden Markov Model (HDP-HMM) by defining a similarity kernel function
on the state space, and scaling transition probabilities by pairwise
similarities. Equivalently, the unnormalized transition weights are
independent Gamma variates, whose shape parameters are as in the
HDP-HMM, and whose scale parameters are obtained by evaluating the
similarity kernel at the pair of states being transitioned
between. This induces a global correlation structure over the
transition probabilities based on the topology induced by the
similarity kernel. We call this model the Hierarchical Dirichlet
Process Hidden Markov Model with Local Transitions
(HDP-HMM-LT). Unfortunately the conditional posterior of the
transition distributions are no longer DPs, due to the varying scale
parameters. We present an alternative representation of this process
as the marginalization of a Markov Jump Process in which: (1) some
jump attempts fail, and (2) the probability of success is proportional
to the similarity between the source and destination states. By
marginalizing out the unsuccessful jumps and the holding times, we
obtain the transition process for the HDP-HMM-LT.  When these
variables are reintroduced as auxiliary data, conditional conjugacy is
restored, admitting exact Gibbs sampling.  As an added benefit, even
without the LT modification, conditioning on the holding times
simplifies inference for the concentration parameters of the HDP, and
allows immediate generalization to Semi-Markov dynamics without
additional data augmentation.  We evaluate the model and inference
scheme on both synthetic data and on a collection of speech separation
data sets in which speakers form conversational groups.  Our model
compares favorably to the HDP-H(S)MM when the data has a local
transition property, without suffering in performance when the data is
generated directly from the comparison model.
