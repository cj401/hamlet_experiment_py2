\section{Experiments}
\label{sec:experiments}

The parameter space for the hidden states, the associated prior $H$ on
$\btheta$, and the similarity function
$\Phi$, is application-specific, but we consider here the case where a state
consists of a finite $D$-dimensional binary vector, $\eta_j$, the similarity
function is a Laplacian kernel defined with respect to Hamming
distance between pairs $\eta_j$ and $\eta_{j'}$, with decay parameter
$\lambda$, and the emission distribution is linear-Gaussian, with $D
\times K$ weight matrix $\bW$, so that each $K$-dimensional
observation is $\Norm{\bW \eta_j}{\Sigma}$.  For all experiments, we 
will assume that $\Sigma$ does not depend on $j$, but this assumption
is easily relaxed if appropriate.  For finite-length binary vector
states, the set of possible states is finite, and so on its face it may
seem that a nonparametric model is unnecessary.  However, if $D$ is
reasonably large, it is likely that most of the $2^D$ possible states
are vanishingly unlikely (and, in fact, the number of observations may
well be less than $2^D$), and so we would like a model that encourages
the selection of a sparse set of states.  Moreover, there may be more
than one state with the same $\theta$, but with different transition
dynamics.  Before describing individual experiments, we describe the additional
inference steps needed for these variables.

\subsection{Additional Inference Steps}
\label{sec:sampling-eta}

\paragraph{Sampling $\eta$} We put independent Beta-Bernoulli priors
on the dimensions of $\eta$.  We Gibbs sample each coordinate $\eta_{jd}$
conditioned on all the others and the coordinate-wise prior means,
$\{\mu_d\}$, which we sample in turn conditioned on the $\eta$s.

\paragraph{Sampling \texorpdfstring{$\lambda$}{kernel decay rate}}
\label{sec:sampling-lambda}
The Laplacian kernel $\Phi$ is defined as $\Phi(\eta_j, \eta_{j'}) =
e^{-\lambda \norm{\eta_j - \eta_{j'}}}$, where in our case the norm is
  Hamming distance.  The parameter $\lambda$ governs the connection between $\btheta$ and
$\bphi$.  Writing \eqref{eq:65} in terms of $\lambda$ and the distance matrix
$\boldsymbol{\Delta}$ gives the likelihood
\begin{equation}
  \label{eq:88}
  p(\bz, \bQ \given \lambda, \btheta) \propto \prod_{j}\prod_{j'}
  e^{-\lambda \Delta_{jj'} n_{jj'}}(1-e^{-\lambda\Delta_{jj'}})^{q_{jj'}} 
\end{equation}
We put an $\Exp{b_{\lambda}}$ prior on $\lambda$, which yields a
posterior density
\begin{equation}
  \label{eq:88}
  p(\lambda \given \bz, \bQ, \btheta) \propto
  e^{-(b_{\lambda} + \sum_{j}\sum_{j'} \Delta_{jj'} n_{jj'})\lambda} \prod_{j}\prod_{j'}
  (1-e^{-\lambda\Delta_{jj'}})^{q_{jj'}}
\end{equation}
This density is log-concave, and so we use Adaptive Rejection Sampling \cite{gilks1992adaptive}
to sample from it.

\paragraph{Sampling \texorpdfstring{$\bW$}{weights} and
  \texorpdfstring{$\Sigma$}{emission covariance}}
Conditioned on the state matrix $\btheta$ and the data matrix $\bY$, 
the weight matrix $\bW$ can be sampled as well using standard methods
for Bayesian linear regression.  We place a zero mean Normal prior on each
element of $\bW$ (including a row of intercept terms), resulting in a
multivariate Normal posterior for each column.  For the experiments
reported below, we constrain $\Sigma$ to be a diagonal matrix, and
place an Inverse Gamma prior on the variances, resulting in conjugate updates.
