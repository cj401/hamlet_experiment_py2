We thank the reviewers for their insightful comments, and valuable suggestions. On a couple of points, our paper gives a false impression about our results; most egregiously, due to a LaTeX error, Figure 4 in the paper is a duplicate of Figure 2, and so the key contrast between the first and second experiment was unfortunately lost. (We had attempted to clarify this in a "Note" added shortly after submissions closed, but the reviewers may not have had access to this, or been aware that it was there.) We apologize for this oversight. In addition, there appears to be a misunderstanding about the properties of the prior on the transition matrix. We aim to briefly correct the record on these two points in this response. Finally, the reviewers offer excellent suggestions for ways to extend and further generalize our model, some of which we are already working on. However, we hope to convince the reviewers that the existing model is useful for an important class of time series inference problems.

As mentioned, Figure 4 is erroneously duplicating Figure 2, and hence it appeared that our model uses more states than an ordinary HDP-HMM, even when the latter model is correctly specified. Reviewer 1 worries, very reasonably, given the incorrect figure, whether the results indicate whether local transitions are present, and Reviewer 2 notes that "the LT model achieves a similar log likelihood to the non LT model but uses many more states." What the correct figure shows is that our model discovers the same number of states as the simpler model (about 5, which is the actual number in the generated data). The relevant curves fall right on top of each other. The LT model has a high degree of posterior uncertainty regarding the value of lambda, with a mean below 1.0, in sharp contrast to the analogous figure in the diarization experiment, where the posterior has a higher mean and very small variance. We have run this experiment with many other datasets using varying numbers of states, and find that this behavior is robust. As such, since our model reduces to an HDP-HMM when lambda is 0, we argue that it can be used in settings where local transition behavior is suspected, but not certain to be present. Again, we apologize for our mistake.

Second, Reviewer 3 comments that 'While the current model assumes that a transition from \theta_i to \theta_j is equally probable to a transition from \theta_j to \theta_i ..., some state might be a "ground" state that can be reached from many states easily.' However, our model does not assume that the transition matrix itself is symmetric; rather that the similarity matrix used to rescale the HDP-generated transition matrix is symmetric.  We should have made this clearer. As such, if there are ground states easily reached from many other states, our model can still discover this, by assigning a high weight to such states in the top level (\beta) of the HDP. The possibility of such behavior is reason to use a similarity kernel (such as the exponential we use) that does not decay too aggressively for large distances, so that the HDP can still override the LT component as needed. Perhaps an even better choice would be a function with hinges, which reaches a constant minimum value beyond a certain distance.

Finally, the reviewers point out some excellent extensions and additional comparisons. Reviewer 3 notes that the Markov Indian Buffet Process (the infinite Factorial HMM) would be a good model to compare against, in addition to or instead of the finite Factorial HMM. Indeed, one of the things we are planning for a future paper is to adapt the HDP-HMM-LT model to accommodate an unbounded number of latent dimensions, thus combining the strengths of the iFHMM and the HDP-HMM-LT. We see the dimension (finite or infinite) of the latent state space to be separate from the whether the dimensions have correlated dynamics (in the vanilla FHMM and iFHMM, the chains evolve a priori independently, whereas our model can discover a state space representing a subset of combinations much smaller than the maximum number possible). As to whether we should have compared against an iFHMM in this paper, we unsure how illuminating that would be, as the (finite) FHMM we did compare against was given the correct dimension up front, which should give it a leg up compared to an iFHMM.

Reviewer 3 also makes the excellent suggestion of putting a prior (perhaps a GP) on the rescaling factors, allowing the model to learn the similarity topology of the latent state space. We agree that this would be an exciting generalization, and indeed is one that we have on our (long) list of future "features". However, we stress that there are applications (such as the speaker diarization application) where it is clear a priori what constitutes a "local" transition --- namely, the latent features overlap. This is a key difference between the non-sequential DILN model and our model: in the HMM setting, instead of "distances" being between entities of two different kinds (e.g., topics and documents), distances are between like entities (latent states), and it is clearer what those distances should look like. In these cases, allowing the locations in the similarity space to float freely would not take full advantage of this prior knowledge.
