We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between ``similar'' states.  This is accomplished by defining a similarity function on the state space, and scaling transition probabilities by pairwise similarities. This induces a global correlation structure over the transition probabilities based on the geometry induced by the similarity function.  We call this model the Hierarchical Dirichlet Process Hidden Markov Model with Local Transitions (HDP-HMM-LT). Since the conditional posterior of the transition distributions is no longer conjugate, we present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. When holding times and failed transitions are reintroduced during inference, conditional conjugacy is restored, admitting exact Gibbs sampling.  Even without the LT modification, conditioning on the holding times simplifies inference for the concentration parameters of the HDP, and allows immediate generalization to Semi-Markov dynamics without additional data augmentation.  We evaluate the model and inference method on a speaker diarization task and a ``harmonic parsing'' task using four-part chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.
