\section{Introduction and Background}

The conventional Hierarchical Dirichlet Process Hidden Markov Model
(HDP-HMM) \cite{beal2001infinite, teh2006hierarchical} 
is a generalization of the conventional finite-state Hidden Markov
Model to allow a countably infinite state space.  The rows of the
infinite transition matrix are coupled by the prior 
through a dependence on a common, discrete base measure,
which is itself drawn from a Dirichlet Process (DP).
The hierarchical structure ensures that, despite the infinite state
space, a common set of destination states will be reachable with high probability
from each source state.  The generative process for the HDP-HMM is the following.

Each state, indexed by $j$, has parameters,
$\theta_j$, drawn from a base measure, $H$.  A top-level
sequence of state weights, $\bbeta = (\beta_1, \beta_2, \dots)$, is
drawn by iteratively breaking a ``stick'' off of the remaining weight
according to a $\Be{1,\gamma}$ distribution.
The parameter $\gamma > 0$ is known as the concentration parameter and
governs how quickly the weights tend to decay, with 
large $\gamma$ corresponding to slow decay,
and hence more weights needed before a given cumulative weight is
reached.  This stick-breaking process is denoted by $\mathsf{GEM}$.
We thus have
\begin{equation}
\theta_j \stackrel{i.i.d.}{\sim} H \qquad \bbeta \sim \GEM{\gamma}
\end{equation}
The actual transition distribution, $\bpi_j$, from state $j$,
is drawn from a DP with concentration $\alpha$ and base measure $\beta$:
\begin{equation}
  \label{eq:1}
  \bpi_j \stackrel{i.i.d}{\sim} \DP{\alpha \bbeta} \qquad j = 1, 2, \dots
\end{equation}
The hidden state sequence is then generated according to the $\bpi_j$.
Let $z_t$ be the index of the chain's state at time $t$.  Then we have
\begin{equation}
  \label{eq:4}
  z_t \given z_{t-1}, \bpi_{z_{t-1}} \sim \Cat{\bpi_{z_{t-1}}} \qquad t = 1, 2, \dots, T
\end{equation}
where $T$ is the length of the data sequence.  Finally, the emission distribution 
for state $j$ is a function of $\theta_j$, so that observation
$\by_t$ is drawn according to
\begin{equation}
  \label{eq:5}
  y_t \given z_{t}, \theta_{z_t} \sim F(\theta_{z_t})
\end{equation}

A shortcoming of the HDP prior on the transition matrix 
is that it does not take into account the fact that the set of source states is the same
as the set of destination states: that is, the distribution $\bpi_j$
has an element which corresponds to state $j$.  Put another way, there
is no special treatment of the diagonal of the transition matrix, so
that self-transitions are no more likely {\it a priori} than
transitions to any other state.  The Sticky HDP-HMM \cite{fox2008hdp}
addresses this issue by adding an extra mass at location $j$ to the base
measure of the DP that generates $\bpi_j$.  That is, \eqref{eq:1} is replaced by
\begin{equation}
  \label{eq:6}
  \bpi_j \sim \DP{\alpha\beta + \kappa \delta_j}.
\end{equation}
An alternative approach that treats self-transitions as special 
is the HDP Hidden Semi-Markov Model (HDP-HSMM;
\citet{johnson2013bayesian}), wherein state duration distributions are modeled
separately, and ordinary self-transitions are ruled out.  In both the
Sticky HDP-HMM and the HDP-HSMM, auxiliary latent variables are introduced to simplify
conditional posterior distributions and facilitate Gibbs sampling.
However, while both of these models have the ability to privilege
self-transitions, they contain no notion of
similarity for pairs of states that are not identical: 
in both cases, when the transition matrix
is integrated out, the prior probability of
transitioning to state $j'$ depends only on the top-level stick
weight associated with state $j'$, and not on the identity or
parameters of the previous state $j$.

The main contribution of this paper is a generalization of the HDP-HMM that allows for a
similarity structure to be defined on the latent state space, so that
``similar'' states are {\em a priori} more likely to have transitions between
them.  This is accomplished by elementwise rescaling and then
renormalizing the HDP transition matrix.  We call this model the
HDP-HMM with Local Transitions (HDP-HMM-LT).  Two versions of the
similarity structure are illustrated: in one case, two states are
similar to the extent that their emission distributions are similar.
In another, the similarity structure is inferred separately.
In both cases, we give augmented data representations that restore
conditional conjugacy and thus allow a simple Gibbs sampling algorithm
to be used for inference.

A rescaling and renormalization approach similar to the one used in
the HDP-HMM-LT is used by \citet{paisley2012discrete} to define their
Discrete Infinite Logistic Normal (DILN) model, an instance of a 
correlated random measure \cite{ranganath2016correlated}, in the setting of
topic modeling.  There, however, the contexts and the mixture components
(topics) are distinct sets, and there is no notion of temporal
dependence.  In addition, \citet{paisley2012discrete} employ variational
inference, whereas we present a Gibbs sampler.  

The paper is structured as follows: In section \ref{sec:model} we
define the HDP-HMM-LT.  In section \ref{sec:inference}, we develop a
straightforward Gibbs sampling algorithm 
based on an augmented data representation, which we call the Markov
Jump Process with Failed Transitions (MJP-FT).  In section
\ref{sec:experiments} we test two versions of the model: one
on a speaker diarization task in which the speakers are
inter-dependent, and another on a four-part chorale corpus,
demonstrating performance improvements over state-of-the-art 
models when ``local transitions''
are more common in the data.  Using sythetic data from an HDP-HMM, we
show that the LT variant can learn not to use its similarity bias when
the data does not support it.  Finally, in section \ref{sec:discussion}, we 
conclude and discuss the relationships between the HDP-HMM-LT and
existing HMM variants.